# -*- coding: utf-8 -*-
"""rag_pipeline.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16dY6oiiudXYQPDSNNUigDqLNRkr_ndEe
"""

!pip install pdfplumber
!pip install langchain
!pip install sentence-transformers
!pip install faiss-cpu
!pip install scikit-learn
!pip install numpy

import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
import faiss
import numpy as np
import pickle
from PIL import Image
import io

!pip install -U sentence-transformers
from huggingface_hub import login

# Login using your HF token (get it from https://huggingface.co/settings/tokens)
login()

# === STEP 1: Load PDF Text with Bounding Box Metadata ===
def load_pdf_text_with_metadata(file_path):
    page_data = []
    with pdfplumber.open(file_path) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            words = page.extract_words()
            full_text = page.extract_text()

            # Save raw image of the page
            image_stream = io.BytesIO()
            page_image = page.to_image(resolution=150).original
            page_image.save(image_stream, format='PNG')

            page_data.append({
                "page_num": page_num,
                "text": full_text.strip() if full_text else "",
                "words": words,
                "image_bytes": image_stream.getvalue()
            })
    return page_data

# === STEP 2: Chunk Text and Track Bounding Boxes ===
def chunk_text_with_metadata(page_data, chunk_size=800, chunk_overlap=100):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ".", " "]
    )

    all_chunks = []
    for page in page_data:
        page_text = page["text"]
        page_num = page["page_num"]
        words = page["words"]
        image_bytes = page["image_bytes"]

        if not page_text:
            continue

        chunks = splitter.create_documents([page_text])
        offset = 0

        for i, chunk in enumerate(chunks):
            chunk_text = chunk.page_content
            start_idx = page_text.find(chunk_text, offset)
            end_idx = start_idx + len(chunk_text)
            offset = end_idx  # To avoid overlapping match with earlier chunk

            # Estimate coordinates based on matching word positions
            chunk_words = [w for w in words if start_idx <= w["doctop"] <= end_idx]
            if chunk_words:
                x0 = min(float(w['x0']) for w in chunk_words)
                x1 = max(float(w['x1']) for w in chunk_words)
                top = min(float(w['top']) for w in chunk_words)
                bottom = max(float(w['bottom']) for w in chunk_words)
                bbox = (x0, top, x1, bottom)
            else:
                bbox = None

            all_chunks.append({
                "content": chunk_text,
                "metadata": {
                    "page_num": page_num,
                    "chunk_index": i,
                    "char_range": (start_idx, end_idx),
                    "bbox": bbox,
                    "image_bytes": image_bytes  # Optional – can skip if not needed
                }
            })

    return all_chunks

!rm -r ~/.cache/huggingface/hub/models--sentence-transformers--paraphrase-mpnet-base-v2

import faiss
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
import pickle

def embed_and_index_chunks(chunks_with_meta, model_name="paraphrase-mpnet-base-v2"):
    model = SentenceTransformer(model_name)
    texts = [chunk["content"] for chunk in chunks_with_meta]
    embeddings = model.encode(texts, show_progress_bar=True)
    embeddings = normalize(embeddings, axis=1)

    index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings)

    # Save for later use (optional)
    with open("SOP.pkl", "wb") as f:
        pickle.dump({
            "chunks": chunks_with_meta,
            "embeddings": embeddings
        }, f)

    return model, index, chunks_with_meta

def search(query, model, index, chunks_with_meta, top_k=3, min_score=0.3, show_image=False):
    query_vec = model.encode([query])
    query_vec = normalize(query_vec, axis=1).astype("float32")
    scores, indices = index.search(query_vec, top_k)

    retrieved_chunks = []

    print(f"\n🔍 Top {top_k} results for: '{query}'\n")
    for rank, (score, idx) in enumerate(zip(scores[0], indices[0])):
        if score < min_score:
            print(f"🔸 Rank {rank+1}: Skipped (Low Score: {score:.2f})")
            continue

        chunk = chunks_with_meta[idx]
        metadata = chunk["metadata"]
        print(f"🔹 Rank {rank+1} (Score: {score:.2f}) | Page {metadata['page_num']} | Chunk {metadata['chunk_index']}")
        print(f"📍 Char Range: {metadata['char_range']} | 📦 BBox: {metadata['bbox']}")
        print(f"{chunk['content'][:700]}...\n")
        retrieved_chunks.append(chunk["content"])

    return retrieved_chunks  # For use in final answer generator

!pip install -U transformers

# === ADDITIONAL IMPORTS ===
from transformers import pipeline

# === INITIALIZE TinyLlama PIPELINE ===
tiny_llama = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")

!pip install -U transformers
from huggingface_hub import login
login(new_session=False)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b-it")

# === STEP 4: Modified Search with LLM Summarization ===
def search(query, model, index, chunks_with_meta, top_k=3, min_score=0.3, show_image=False):
    from PIL import ImageDraw

    query_vec = model.encode([query])
    query_vec = normalize(query_vec, axis=1).astype("float32")
    scores, indices = index.search(query_vec, top_k)

    selected_chunks = []
    print(f"\n🔍 Top {top_k} results for: '{query}'\n")
    for rank, (score, idx) in enumerate(zip(scores[0], indices[0])):
        if score < min_score:
            print(f"🔸 Rank {rank+1}: Skipped (Low Score: {score:.2f})")
            continue

        chunk = chunks_with_meta[idx]
        content = chunk.get("content")

        if not content or not isinstance(content, str):
            print(f"⚠️ Skipped chunk at index {idx} due to missing or invalid content.")
            continue

        metadata = chunk["metadata"]
        print(f"🔹 Rank {rank+1} (Score: {score:.2f}) | Page {metadata['page_num']} | Chunk {metadata['chunk_index']}")
        print(f"📍 Char Range: {metadata['char_range']} | 📦 BBox: {metadata['bbox']}")
        print(f"{content[:500]}...\n")
        selected_chunks.append(content)


        if show_image and metadata["bbox"]:
            img = Image.open(io.BytesIO(metadata["image_bytes"]))
            draw = ImageDraw.Draw(img)
            draw.rectangle(metadata["bbox"], outline="red", width=2)
            img.show()

    # 🔥 Combine Query + Chunks into TinyLlama Prompt
    combined_context = "\n".join(selected_chunks)
    prompt = f"""You are a helpful assistant. Based on the following context and query, respond only in bullet points in a clean, step-wise format:

Context:
{combined_context}

Query:
{query}

Answer in steps:"""

    # 🔥 Call TinyLlama for final summarization
    # Make sure tiny_llama pipeline is initialized before calling this function
    result = tiny_llama(prompt, max_new_tokens=300, do_sample=False)[0]["generated_text"]

    # 🔥 Display final result
    print("\n🧠 Final Answer (Generated by TinyLlama):\n")
    bullet_output = result.split("Answer in steps:")[-1].strip()
    print(bullet_output)

from google.colab import files

uploaded_files = files.upload()  # Upload multiple PDFs

all_chunks = []
for filename in uploaded_files.keys():
    print(f"📄 Processing: {filename}")
    page_data = load_pdf_text_with_metadata(filename)
    chunks_with_meta = chunk_text_with_metadata(page_data)
    all_chunks.extend(chunks_with_meta)

# === Embed and Index ===
embedding_model, faiss_index, all_chunks = embed_and_index_chunks(all_chunks)

# === Interactive Loop ===
while True:
    user_query = input("❓ Enter your query (or type 'exit' to quit): ")
    if user_query.lower() == "exit":
        break
    search(user_query, embedding_model, faiss_index, all_chunks, show_image=False)